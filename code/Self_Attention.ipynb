{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow code for Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referrence:\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "* Tensor2tensor (https://github.com/tensorflow/tensor2tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fun(**config):\n",
    "    data = tf.random_normal((\n",
    "        config['batch_size'], config['sequence_length'], config['hidden_dim']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_fun(Q, K, scaled_=True, masked_=False):\n",
    "    attention = tf.matmul(Q, K, transpose_b=True)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if scaled_:\n",
    "        d_k = tf.cast(tf.shape(K)[-1], dtype=tf.float32)\n",
    "        attention = tf.divide(attention, tf.sqrt(d_k))  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if masked_:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    attention = tf.nn.softmax(attention, dim=-1)  # [batch_size, sequence_length, sequence_length]\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fun(data, **config):\n",
    "    Q = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    K = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    V = tf.layers.dense(data, config['n_classes'])  # [batch_size, sequence_length, n_classes]\n",
    "\n",
    "    attention = attention_fun(Q, K)  # [batch_size, sequence_length, sequence_length]\n",
    "    output = tf.matmul(attention, V)  # [batch_size, sequence_length, n_classes]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.4977258   0.29166442  0.00683167 ... -0.94390374  2.132048\n",
      "    0.10638817]\n",
      "  [ 0.19402815  0.7921161   0.80763364 ... -1.4685776  -1.7131332\n",
      "    0.6364754 ]\n",
      "  [-0.8855895   0.2456611  -1.2027229  ...  0.8814262   0.70316225\n",
      "    0.2799854 ]\n",
      "  ...\n",
      "  [ 0.1501507   0.9974407  -0.39481825 ...  0.65158623  0.85993665\n",
      "    0.38736168]\n",
      "  [ 0.10191716 -0.10676987  0.44088697 ...  2.0026267   1.1333106\n",
      "   -0.5815502 ]\n",
      "  [ 0.17985587 -0.8863931  -0.46068347 ...  0.64572054  0.9446574\n",
      "   -0.50968754]]\n",
      "\n",
      " [[-0.93790776  0.08903116 -1.7041032  ...  1.1371131   0.4319286\n",
      "    0.8350716 ]\n",
      "  [-0.18624283  1.282129    0.6841612  ...  0.3939439  -2.1237617\n",
      "    1.065678  ]\n",
      "  [-1.1238523   1.658433   -1.209692   ... -0.02763248 -0.3910349\n",
      "    1.0298834 ]\n",
      "  ...\n",
      "  [-0.59902745 -0.7648693  -0.5120379  ...  1.1642501  -1.4624057\n",
      "   -1.2281034 ]\n",
      "  [-1.4802809   0.15423995 -0.27076465 ...  1.5185727  -0.12518252\n",
      "   -0.76886964]\n",
      "  [-1.4296391   0.1306755  -1.2566013  ...  0.24928606 -0.51026887\n",
      "    1.8233235 ]]\n",
      "\n",
      " [[ 0.45047292  1.5891871  -0.6422995  ...  0.0314796   0.7042724\n",
      "   -0.50083965]\n",
      "  [ 0.54973525 -0.05468847  0.3761202  ... -0.36627036  0.2020691\n",
      "    0.6536794 ]\n",
      "  [-0.30707192  0.1886874   1.8294896  ...  0.86299837 -0.20829272\n",
      "   -1.270076  ]\n",
      "  ...\n",
      "  [ 1.1034789   0.78178746 -0.08530989 ... -0.5700897   0.20262201\n",
      "    0.03883687]\n",
      "  [ 0.15575671 -0.7621213   1.6444463  ...  0.15582132 -0.18952386\n",
      "    0.00793487]\n",
      "  [-0.21447107  2.8293993   0.22755393 ...  0.27422547 -0.22059816\n",
      "   -0.41432637]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.11459722 -1.4636991   0.03250316 ...  0.15387322  1.6879379\n",
      "   -1.5153637 ]\n",
      "  [ 0.3377739   1.2172447  -1.0317985  ... -2.0773833   1.9175586\n",
      "   -0.51617444]\n",
      "  [-1.4466435   0.39124084  0.8125402  ...  1.1649336  -0.36108685\n",
      "    0.5648907 ]\n",
      "  ...\n",
      "  [-1.0243691   0.6737777  -0.33171985 ... -0.54097897  1.4165946\n",
      "    1.5602732 ]\n",
      "  [-1.1161935  -0.07191473 -0.4211589  ... -2.3658845  -0.535617\n",
      "   -1.4098824 ]\n",
      "  [-0.35435367 -2.8878548   0.75079006 ...  0.41371593  0.51326245\n",
      "    1.187496  ]]\n",
      "\n",
      " [[ 0.10156222 -1.2138995  -0.04728205 ... -1.3227149  -2.0515034\n",
      "   -1.8753735 ]\n",
      "  [-0.2983022   0.04451064  0.36375228 ...  0.0128226   0.27998012\n",
      "    0.84288234]\n",
      "  [ 0.22631674 -1.4800739   0.15177491 ... -1.2652231   0.57544196\n",
      "   -0.7835504 ]\n",
      "  ...\n",
      "  [ 1.0849334  -1.1692832   0.93580985 ...  0.3910831  -1.10298\n",
      "   -1.0450783 ]\n",
      "  [ 0.49577054 -1.8137575   3.8128886  ...  0.15608388 -0.59272504\n",
      "    0.83565974]\n",
      "  [-0.83710456  0.17781463 -1.9569323  ...  1.1329184  -0.65527385\n",
      "   -1.4951522 ]]\n",
      "\n",
      " [[-0.29576528  0.15790819  1.3107466  ...  0.35712945 -1.3461813\n",
      "   -0.01270631]\n",
      "  [ 1.4927008  -0.86213976 -1.0364542  ...  0.4800533   1.3441252\n",
      "   -2.4095204 ]\n",
      "  [ 1.133121    0.9429481   0.22474553 ... -0.46962348  2.9061723\n",
      "   -1.2689551 ]\n",
      "  ...\n",
      "  [ 0.47115728 -0.05505516  0.31847602 ...  1.2182393  -1.2089472\n",
      "   -0.6273504 ]\n",
      "  [-0.612544   -1.7362403   0.25567058 ... -0.34797606 -2.1272655\n",
      "    0.8729822 ]\n",
      "  [ 0.1944924   0.14111616 -1.255298   ...  1.0244235   1.1207293\n",
      "   -0.2155656 ]]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    inputs = input_fun(batch_size=32, sequence_length=10, hidden_dim=128)\n",
    "    with tf.Session() as sess:  print(inputs.eval())\n",
    "    #outputs = model_fun(inputs, hidden_dim=128, n_classes=2)\n",
    "\n",
    "    #with tf.Session() as sess:\n",
    "    #    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    #    outputs_ = sess.run(outputs)\n",
    "    #    print(outputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal_2:0' shape=(32, 10, 128) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"random_normal\"\n",
      "op: \"Add\"\n",
      "input: \"random_normal/mul\"\n",
      "input: \"random_normal/mean\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
