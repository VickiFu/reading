{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow code for Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referrence:\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "* Tensor2tensor (https://github.com/tensorflow/tensor2tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fun(**config):\n",
    "    data = tf.random_normal((\n",
    "        config['batch_size'], config['sequence_length'], config['hidden_dim']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_fun(Q, K, scaled_=True, masked_=False):\n",
    "    attention = tf.matmul(Q, K, transpose_b=True)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if scaled_:\n",
    "        d_k = tf.cast(tf.shape(K)[-1], dtype=tf.float32)\n",
    "        attention = tf.divide(attention, tf.sqrt(d_k))  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if masked_:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    attention = tf.nn.softmax(attention, dim=-1)  # [batch_size, sequence_length, sequence_length]\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fun(data, **config):\n",
    "    Q = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    K = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    V = tf.layers.dense(data, config['n_classes'])  # [batch_size, sequence_length, n_classes]\n",
    "\n",
    "    attention = attention_fun(Q, K)  # [batch_size, sequence_length, sequence_length]\n",
    "    output = tf.matmul(attention, V)  # [batch_size, sequence_length, n_classes]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0415 11:06:47.832066 4544970176 deprecation.py:323] From <ipython-input-4-2d2c1bc57284>:2: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0415 11:06:47.835134 4544970176 deprecation.py:506] From /Users/vfu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0415 11:06:48.140724 4544970176 deprecation.py:506] From <ipython-input-3-28e7cc9547e6>:11: calling softmax (from tensorflow.python.ops.nn_ops) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    inputs = input_fun(batch_size=32, sequence_length=10, hidden_dim=128)\n",
    "    #with tf.Session() as sess:  print(inputs.eval())\n",
    "    outputs = model_fun(inputs, hidden_dim=128, n_classes=2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        outputs_ = sess.run(outputs)\n",
    "        print(outputs_.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
