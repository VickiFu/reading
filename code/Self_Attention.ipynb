{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow code for Self-Attention "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referrence:\n",
    "* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
    "* [Transformer: A Novel Neural Network Architecture for Language Understanding](https://research.googleblog.com/2017/08/transformer-novel-neural-network.html)\n",
    "* Tensor2tensor (https://github.com/tensorflow/tensor2tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fun(**config):\n",
    "    data = tf.random_normal((\n",
    "        config['batch_size'], config['sequence_length'], config['hidden_dim']))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_fun(Q, K, scaled_=True, masked_=False):\n",
    "    attention = tf.matmul(Q, K, transpose_b=True)  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if scaled_:\n",
    "        d_k = tf.cast(tf.shape(K)[-1], dtype=tf.float32)\n",
    "        attention = tf.divide(attention, tf.sqrt(d_k))  # [batch_size, sequence_length, sequence_length]\n",
    "\n",
    "    if masked_:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    attention = tf.nn.softmax(attention, dim=-1)  # [batch_size, sequence_length, sequence_length]\n",
    "    return attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fun(data, **config):\n",
    "    Q = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    K = tf.layers.dense(data, config['hidden_dim'])  # [batch_size, sequence_length, hidden_dim]\n",
    "    V = tf.layers.dense(data, config['n_classes'])  # [batch_size, sequence_length, n_classes]\n",
    "\n",
    "    attention = attention_fun(Q, K)  # [batch_size, sequence_length, sequence_length]\n",
    "    output = tf.matmul(attention, V)  # [batch_size, sequence_length, n_classes]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 8.64741611e+00  4.31900692e+00 -9.97002125e-02 ...  1.65095139e+00\n",
      "    2.85142422e+01 -1.14556618e+01]\n",
      "  [-3.04894686e+00  3.77915645e+00  9.54752541e+00 ... -2.43744087e+01\n",
      "    2.64614868e+00  6.24607468e+00]\n",
      "  [-6.80655432e+00  2.43296318e+01 -1.13980789e+01 ...  1.79394302e+01\n",
      "    9.53603935e+00 -1.17955828e+01]\n",
      "  ...\n",
      "  [-1.68654323e-02 -1.27606792e+01  6.51028395e-01 ... -2.44753838e+00\n",
      "    6.09450722e+00  9.07594490e+00]\n",
      "  [-6.79663181e+00  1.92100763e-01 -6.65590811e+00 ...  2.99443665e+01\n",
      "    2.50837660e+00 -1.48180513e+01]\n",
      "  [ 7.70586395e+00 -3.26023459e+00  2.29179263e-02 ... -3.32949114e+00\n",
      "    2.43676496e+00  7.07141685e+00]]\n",
      "\n",
      " [[-5.38709736e+00 -9.43058968e-01  1.58931513e+01 ... -8.20481491e+00\n",
      "    5.34416199e-01 -8.28966141e+00]\n",
      "  [-6.71831131e+00 -1.42814913e+01 -2.01013255e+00 ...  3.77998567e+00\n",
      "   -1.75789642e+00  4.20802712e-01]\n",
      "  [ 1.55538101e+01  8.83392715e+00  1.32826500e+01 ...  3.69509029e+00\n",
      "    1.87220726e+01 -1.55943251e+01]\n",
      "  ...\n",
      "  [ 3.87775612e+00  1.35356321e+01  8.78552818e+00 ... -3.46578908e+00\n",
      "    8.63472748e+00 -4.94267654e+00]\n",
      "  [-2.44826007e+00  1.72021351e+01 -1.13789034e+01 ...  3.87110758e+00\n",
      "   -7.86326838e+00 -1.35463438e+01]\n",
      "  [-1.76241837e+01  1.30937207e+00  1.02492456e+01 ...  4.30279827e+00\n",
      "   -8.37391853e+00  5.88214397e+00]]\n",
      "\n",
      " [[ 1.65715301e+00 -2.34555751e-01  4.73404980e+00 ... -3.23024979e+01\n",
      "    8.66884041e+00  1.84069500e+01]\n",
      "  [ 7.57408524e+00  1.48480759e+01  1.15612793e+00 ...  2.47545853e+01\n",
      "    3.43736839e+00 -2.75092125e-01]\n",
      "  [-2.89282823e+00 -1.54363499e+01  2.40053082e+00 ... -7.08614874e+00\n",
      "   -1.19340229e+01 -1.86604843e+01]\n",
      "  ...\n",
      "  [ 2.13725090e-01  4.98191881e+00  6.72888517e-01 ...  8.36761284e+00\n",
      "    1.02838745e+01  6.21468401e+00]\n",
      "  [-1.31218958e+01  8.81272030e+00 -8.89136910e-01 ...  1.10019341e+01\n",
      "   -1.84910202e+01 -7.86793137e+00]\n",
      "  [-1.46826162e+01  2.04374743e+00 -3.90060043e+00 ...  1.87059917e+01\n",
      "   -1.14141855e+01  1.03254061e+01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 9.82446957e+00  2.54892874e+00 -1.01869774e+01 ... -1.19917488e+00\n",
      "   -3.55861163e+00 -5.04467869e+00]\n",
      "  [-6.20517015e-01 -3.26263928e+00 -3.49502754e+00 ...  8.34445190e+00\n",
      "    6.70423508e-01 -1.53338947e+01]\n",
      "  [-1.50064921e+00 -2.54572010e+00  7.38988638e+00 ... -4.21333408e+00\n",
      "    1.36647196e+01  6.60246086e+00]\n",
      "  ...\n",
      "  [-2.65321674e+01 -1.39504833e+01  2.60475731e+01 ... -4.06014729e+00\n",
      "    1.23845320e+01  5.74551105e+00]\n",
      "  [ 1.30579557e+01  1.22808237e+01 -8.92340279e+00 ... -2.43155766e+01\n",
      "   -1.45201099e+00 -9.19512463e+00]\n",
      "  [ 5.60019875e+00 -5.05207634e+00  5.18160534e+00 ... -2.48378544e+01\n",
      "   -1.54560959e+00 -2.22721748e+01]]\n",
      "\n",
      " [[ 1.15707362e+00 -7.75178909e+00  1.43071022e+01 ...  1.83256950e+01\n",
      "   -5.02921104e-01  1.01969919e+01]\n",
      "  [ 9.75646782e+00 -2.81672335e+00 -1.40243311e+01 ...  1.35369968e+00\n",
      "    1.20308533e+01  3.18132591e+00]\n",
      "  [ 1.41613750e+01 -2.76024532e+00  1.15148010e+01 ... -1.51955757e+01\n",
      "    4.60160255e-01  2.50271034e+01]\n",
      "  ...\n",
      "  [-4.02638435e+00  3.87683535e+00 -2.53743505e+00 ...  5.02407455e+00\n",
      "    2.06279793e+01 -1.22651081e+01]\n",
      "  [ 9.41432953e+00 -9.66359302e-02  8.37933540e+00 ... -1.65504208e+01\n",
      "   -9.58502388e+00  1.75393925e+01]\n",
      "  [-6.45649052e+00 -6.75874519e+00  1.65609016e+01 ...  6.35415363e+00\n",
      "   -1.55690527e+01  1.40060711e+01]]\n",
      "\n",
      " [[-2.22162800e+01  1.60307026e+00 -3.11534810e+00 ... -1.79079437e+01\n",
      "   -1.15123024e+01 -1.12906437e+01]\n",
      "  [-2.34411621e+00  4.78955555e+00  7.39891005e+00 ... -1.07328272e+01\n",
      "    2.37721443e-01 -8.85629654e-02]\n",
      "  [ 7.69561481e+00  3.10381222e+00 -9.52341175e+00 ...  2.57797408e+00\n",
      "    9.99231339e+00  1.33164482e+01]\n",
      "  ...\n",
      "  [-3.94439316e+00  2.15220881e+00 -1.49960461e+01 ...  1.79774723e+01\n",
      "   -1.84817162e+01 -5.58023262e+00]\n",
      "  [-1.63026428e+01 -2.10170054e+00 -1.05165339e+01 ... -8.03942394e+00\n",
      "   -2.90418549e+01  7.44445276e+00]\n",
      "  [ 7.57019520e+00 -1.30290012e+01  1.03093882e+01 ... -4.42653894e+00\n",
      "    1.89953747e+01  4.75434113e+00]]]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    inputs = input_fun(batch_size=32, sequence_length=10, hidden_dim=128)\n",
    "    #with tf.Session() as sess:  print(inputs.eval())\n",
    "    outputs = model_fun(inputs, hidden_dim=128, n_classes=2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        outputs_ = sess.run(outputs)\n",
    "        print(outputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'random_normal_2:0' shape=(32, 10, 128) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    inputs = input_fun(batch_size=32, sequence_length=10, hidden_dim=128)\n",
    "    #with tf.Session() as sess:  print(inputs.eval())\n",
    "    outputs = model_fun(inputs, hidden_dim=128, n_classes=2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        outputs_ = sess.run(outputs)\n",
    "        print(outputs_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: \"random_normal\"\n",
      "op: \"Add\"\n",
      "input: \"random_normal/mul\"\n",
      "input: \"random_normal/mean\"\n",
      "attr {\n",
      "  key: \"T\"\n",
      "  value {\n",
      "    type: DT_FLOAT\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
